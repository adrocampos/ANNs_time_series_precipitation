{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883ef3a-6a67-4b30-9af1-b488471fdc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from pickle import load, dump\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import KFold\n",
    "from validate import validate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ld(f): return load(open(f, 'rb'))  # _pickle.load\n",
    "def dp(what, fP): dump(what, open(fP, 'wb'))  # _pickle.dump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b1353-9b8f-428a-aa3e-6bb770ceefe6",
   "metadata": {},
   "source": [
    "## Station-specific probabilistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9641e4-5fd0-4f50-a7ec-c1ebeba450f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 20\n",
    "data_dir = './data'\n",
    "locations = ['muOsna', 'wernig', 'braunl', 'redlen']\n",
    "station = locations[2]\n",
    "print(station)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bd14de-a02c-465f-a8a6-0ae0d8329d92",
   "metadata": {},
   "source": [
    "### Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5f40bf-f689-4fb1-9e10-c4ad733c10a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changes according to station. Obtained from arch_search.ipynb\n",
    "archs = {'muOsna': {'depth': 6, 'units': 64},\n",
    "         'wernig': {'depth': 8, 'units': 64},\n",
    "         'braunl': {'depth': 8, 'units': 32},\n",
    "         'redlen': {'depth': 6, 'units': 64},\n",
    "         'general': {'depth': 8, 'units': 64}}\n",
    "\n",
    "best_depth = archs[station]['depth']\n",
    "best_units = archs[station]['units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97835597-cf12-406c-994a-9889a94ed1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the dataset\n",
    "id_predictors   = np.loadtxt(data_dir+'/id_predictors/'+station+'.txt', dtype=np.int32, delimiter=',') - 1 #Matlab index start in 1\n",
    "\n",
    "trn_x = np.load(data_dir + '/' + station + '/trn_x.npy')\n",
    "trn_x = trn_x[:, id_predictors]\n",
    "trn_y = np.load(data_dir + '/' + station + '/trn_y.npy')\n",
    "\n",
    "## Use only days with rain\n",
    "trn_x = trn_x[trn_y > 0]\n",
    "trn_y = trn_y[trn_y > 0]\n",
    "trn_y = np.log(trn_y) ## Log transform for normal output\n",
    "print('Train dataset:', trn_x.shape, '---> ', trn_y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea8449-8471-430a-822f-aad2acb1f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(n_runs):\n",
    "    \n",
    "    print('Run', r+1, '/', n_runs)\n",
    "    \n",
    "    cross_validation_models = []\n",
    "    cross_validation_losses = []\n",
    "    cross_validation_histos = []\n",
    "\n",
    "\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    for train_index, val_index in kf.split(trn_x, trn_y):\n",
    "\n",
    "        batch_size = 64\n",
    "        epochs = 256\n",
    "\n",
    "        optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
    "        negloglik = lambda y, dist: -dist.log_prob(y)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, restore_best_weights=True)\n",
    "\n",
    "        activation = 'relu'\n",
    "\n",
    "        model = tf.keras.Sequential()\n",
    "\n",
    "        model.add(tf.keras.layers.InputLayer(input_shape=(trn_x.shape[1])))\n",
    "\n",
    "        for d in range(best_depth):\n",
    "            model.add(tf.keras.layers.Dense(best_units, activation=activation))\n",
    "\n",
    "        \n",
    "        ## Probabilistic output\n",
    "        model.add(tf.keras.layers.Dense(2))\n",
    "        model.add(tfp.layers.DistributionLambda(lambda t: tfp.distributions.Normal(t[..., :1], 1e-3 + tf.math.softplus(0.05 * t[...,1:]))))\n",
    "        model.compile(optimizer, negloglik)\n",
    "\n",
    "        hist = model.fit(trn_x[train_index], trn_y[train_index], batch_size, epochs, validation_data=(trn_x[val_index], trn_y[val_index]), shuffle=True, verbose=0, callbacks=[early_stop])\n",
    "        loss = model.evaluate(trn_x[val_index], trn_y[val_index], verbose=0)\n",
    "\n",
    "        cross_validation_models.append(model)\n",
    "        cross_validation_losses.append(loss)\n",
    "        cross_validation_histos.append(hist)\n",
    "\n",
    "\n",
    "    ## Pick the best model from cross validation\n",
    "    idx_best = np.argmin(cross_validation_losses)\n",
    "\n",
    "    best_model = cross_validation_models[idx_best]\n",
    "    best_model.save('results/probabilistic_regression/specific/' + station + '/model' + str(r).zfill(2) + '.h5')\n",
    "    best_hist = cross_validation_histos[idx_best]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(best_hist.history['loss'], label='loss')\n",
    "    ax.plot(best_hist.history['val_loss'], label='val_loss')\n",
    "    fig.legend()\n",
    "    \n",
    "    fig.savefig('results/probabilistic_regression/specific/' + station + '/loss' + str(r).zfill(2) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0fa18c-275a-4337-90be-abf32a6df6ab",
   "metadata": {},
   "source": [
    "### Testing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ccfc0-5992-455d-8fc6-c3eba02ca1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_t = np.load(data_dir + '/' + station + '/tst_t.npy')\n",
    "tst_x = np.load(data_dir + '/' + station + '/tst_x.npy')\n",
    "tst_x = tst_x[:, id_predictors]\n",
    "tst_y = np.load(data_dir + '/' + station + '/tst_y.npy')\n",
    "\n",
    "tst_t = tst_t[tst_y > 0]\n",
    "tst_x = tst_x[tst_y > 0]\n",
    "tst_y = tst_y[tst_y > 0]\n",
    "print('Test dataset:', tst_x.shape, '--->', tst_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2332c9-faec-4d75-add3-0e1929acadf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for r in range(n_runs):\n",
    "\n",
    "    model = tf.keras.models.load_model('results/probabilistic_regression/specific/' + station + '/model' + str(r).zfill(2) + '.h5', compile=False)\n",
    "\n",
    "    pred = model(tst_x)\n",
    "    pred = pred.quantile(.5)\n",
    "    pred = np.array([x.numpy() for x in pred])\n",
    "    pred = np.exp(pred) ## Reverse back log transform \n",
    "    pred = np.squeeze(pred)\n",
    "    results.append(pred)\n",
    "\n",
    "results = np.array(results)\n",
    "print('Results [runs x ensembles x predictions] =', results.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060f3f32-cd86-420e-8930-09f5fd6c6323",
   "metadata": {},
   "outputs": [],
   "source": [
    "leps_cosmo = validate([], tst_t, [0], [4], True, station)['LEPS'][9]\n",
    "results_skill = []\n",
    "\n",
    "for r in range(n_runs):\n",
    "    leps       = validate(results[r], tst_t, [0], [4], False, station)['LEPS']\n",
    "    skill = 1 - (leps / leps_cosmo)\n",
    "    results_skill.append(skill)\n",
    "\n",
    "print(station, 'Results:', 'min =', np.min(results_skill), 'median = ', np.median(results_skill), 'max =', np.max(results_skill))\n",
    "np.save('results/probabilistic_regression/specific/' + station + '/results_skill.npy', results_skill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0914e0-eab7-4baa-ac8e-27fe392118f0",
   "metadata": {},
   "source": [
    "## General probabilistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20cecfd-c4d5-4e4e-9644-a4449cf8ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_units = archs['general']['units']\n",
    "best_depth = archs['general']['depth']\n",
    "print(best_units, best_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14204ecd-ec0e-4d33-a2ce-4bad25390e65",
   "metadata": {},
   "source": [
    "### Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0097bfe0-ea14-42be-856a-552e7d9e6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_x = []\n",
    "trn_y = []\n",
    "\n",
    "for station in locations:\n",
    "    \n",
    "    trn_x.append(np.load(data_dir + '/' + station + '/trn_x.npy'))\n",
    "    trn_y.append(np.load(data_dir + '/' + station + '/trn_y.npy'))\n",
    "\n",
    "trn_x = np.concatenate(trn_x)\n",
    "trn_y = np.concatenate(trn_y)\n",
    "\n",
    "## Use only days with rain\n",
    "trn_x = trn_x[trn_y > 0]\n",
    "trn_y = trn_y[trn_y > 0]\n",
    "trn_y = np.log(trn_y) ## Log transform for normal output\n",
    "print('Training dataset:', trn_x.shape, '---> ', trn_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713f1ec3-d0cb-4495-b3df-e17b8dd3794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(n_runs):\n",
    "    \n",
    "    print('Run', r+1, '/', n_runs)\n",
    "    \n",
    "    cross_validation_models = []\n",
    "    cross_validation_losses = []\n",
    "    cross_validation_histos = []\n",
    "\n",
    "\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    for train_index, val_index in kf.split(trn_x, trn_y):\n",
    "\n",
    "        batch_size = 64\n",
    "        epochs = 256\n",
    "\n",
    "        optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
    "        negloglik = lambda y, dist: -dist.log_prob(y)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, restore_best_weights=True)\n",
    "\n",
    "        activation = 'relu'\n",
    "\n",
    "        model = tf.keras.Sequential()\n",
    "\n",
    "        model.add(tf.keras.layers.InputLayer(input_shape=(trn_x.shape[1])))\n",
    "\n",
    "        for d in range(best_depth):\n",
    "            model.add(tf.keras.layers.Dense(best_units, activation=activation))\n",
    "\n",
    "        \n",
    "        ## Probabilistic output\n",
    "        model.add(tf.keras.layers.Dense(2))\n",
    "        model.add(tfp.layers.DistributionLambda(lambda t: tfp.distributions.Normal(t[..., :1], 1e-3 + tf.math.softplus(0.05 * t[...,1:]))))\n",
    "        model.compile(optimizer, negloglik)\n",
    "\n",
    "        hist = model.fit(trn_x[train_index], trn_y[train_index], batch_size, epochs, validation_data=(trn_x[val_index], trn_y[val_index]), shuffle=True, verbose=0, callbacks=[early_stop])\n",
    "        loss = model.evaluate(trn_x[val_index], trn_y[val_index], verbose=0)\n",
    "\n",
    "        cross_validation_models.append(model)\n",
    "        cross_validation_losses.append(loss)\n",
    "        cross_validation_histos.append(hist)\n",
    "\n",
    "\n",
    "    ## Pick the best model from cross validation\n",
    "    idx_best = np.argmin(cross_validation_losses)\n",
    "\n",
    "    best_model = cross_validation_models[idx_best]\n",
    "    best_model.save('results/probabilistic_regression/general/model' + str(r).zfill(2) + '.h5')\n",
    "    best_hist = cross_validation_histos[idx_best]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(best_hist.history['loss'], label='loss')\n",
    "    ax.plot(best_hist.history['val_loss'], label='val_loss')\n",
    "    fig.legend()\n",
    "    \n",
    "    fig.savefig('results/probabilistic_regression/general/loss' + str(r).zfill(2) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a005763f-9522-44d3-9d06-af652cf03664",
   "metadata": {},
   "source": [
    "### Test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e2cc44-8f69-4229-bc36-74ee796b0afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in locations:\n",
    "    \n",
    "    tst_t = np.load(data_dir + '/' + station + '/tst_t.npy')\n",
    "    tst_x = np.load(data_dir + '/' + station + '/tst_x.npy')\n",
    "    tst_y = np.load(data_dir + '/' + station + '/tst_y.npy')\n",
    "    tst_t = tst_t[tst_y > 0]\n",
    "    tst_x = tst_x[tst_y > 0]\n",
    "    tst_y = tst_y[tst_y > 0]\n",
    "    print(tst_x.shape, '--->', tst_y.shape)\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for r in range(n_runs):\n",
    "        model = tf.keras.models.load_model('results/probabilistic_regression/general/model' + str(r).zfill(2) + '.h5', compile=False)\n",
    "        pred = model(tst_x)\n",
    "        pred = pred.quantile(.5)\n",
    "        pred = np.squeeze(pred)\n",
    "        pred = np.exp(pred) ## Reverse back log transform \n",
    "        results.append(pred)\n",
    "\n",
    "    results = np.array(results)\n",
    "    print('Results [runs x ensembles x predictions] =', results.shape)\n",
    "    \n",
    "    leps_cosmo = validate([], tst_t, [0], [4], True, station)['LEPS'][9]\n",
    "    results_skill = []\n",
    "\n",
    "    for r in range(n_runs):\n",
    "        print(r)\n",
    "        leps       = validate(results[r], tst_t, [0], [4], False, station)['LEPS']\n",
    "        skill = 1 - (leps / leps_cosmo)\n",
    "        results_skill.append(skill)\n",
    "\n",
    "    print(station, 'Results:', 'min =', np.min(results_skill), 'median = ', np.median(results_skill), 'max =', np.max(results_skill))\n",
    "    np.save('results/probabilistic_regression/general/' + station + '_results_skill.npy', results_skill)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
