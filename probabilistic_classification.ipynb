{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883ef3a-6a67-4b30-9af1-b488471fdc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from pickle import load, dump\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import KFold\n",
    "from validate import validate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ld(f): return load(open(f, 'rb'))  # _pickle.load\n",
    "def dp(what, fP): dump(what, open(fP, 'wb'))  # _pickle.dump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b1353-9b8f-428a-aa3e-6bb770ceefe6",
   "metadata": {},
   "source": [
    "## Station-specific classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9641e4-5fd0-4f50-a7ec-c1ebeba450f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 20\n",
    "data_dir = './data'\n",
    "locations = ['muOsna', 'wernig', 'braunl', 'redlen']\n",
    "station = locations[3]\n",
    "print(station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1b3148-5d8c-4bec-a6ea-d7439ddbadb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changes according to station. Obtained from arch_search.ipynb\n",
    "archs = {'muOsna': {'depth': 6, 'units': 64},\n",
    "         'wernig': {'depth': 8, 'units': 64},\n",
    "         'braunl': {'depth': 8, 'units': 32},\n",
    "         'redlen': {'depth': 6, 'units': 64},\n",
    "         'general': {'depth': 8, 'units': 64}}\n",
    "\n",
    "best_depth = archs[station]['depth']\n",
    "best_units = archs[station]['units']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bd14de-a02c-465f-a8a6-0ae0d8329d92",
   "metadata": {},
   "source": [
    "### Traininig phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97835597-cf12-406c-994a-9889a94ed1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the dataset\n",
    "id_predictors   = np.loadtxt(data_dir+'/id_predictors/'+station+'.txt', dtype=np.int32, delimiter=',') - 1 #Matlab index start in 1\n",
    "\n",
    "trn_x = np.load(data_dir + '/' + station + '/trn_x.npy')\n",
    "trn_x = trn_x[:, id_predictors]\n",
    "trn_y = np.load(data_dir + '/' + station + '/trn_y.npy')\n",
    "\n",
    "## Boolean rain / no rain\n",
    "trn_y = np.array(trn_y >= 0.1, dtype=float)\n",
    "print('Train dataset:', trn_x.shape, '---> ', trn_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea8449-8471-430a-822f-aad2acb1f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(n_runs):\n",
    "    \n",
    "    print('Run', r+1, '/', n_runs)\n",
    "    \n",
    "    cross_validation_models = []\n",
    "    cross_validation_losses = []\n",
    "    cross_validation_histos = []\n",
    "\n",
    "\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    for train_index, val_index in kf.split(trn_x, trn_y):\n",
    "\n",
    "        batch_size = 64\n",
    "        epochs = 128\n",
    "\n",
    "        optimizer = tf.optimizers.Adam(learning_rate=0.0001)\n",
    "        negloglik = lambda y, dist: -dist.log_prob(y)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5)\n",
    "\n",
    "        activation = 'relu'\n",
    "\n",
    "        model = tf.keras.Sequential()\n",
    "\n",
    "        model.add(tf.keras.layers.InputLayer(input_shape=(trn_x.shape[1])))\n",
    "\n",
    "        for d in range(best_depth):\n",
    "            model.add(tf.keras.layers.Dense(best_units, activation=activation))\n",
    "        \n",
    "        ## Probabilistic output\n",
    "        model.add(tf.keras.layers.Dense(1))\n",
    "        model.add(tfp.layers.DistributionLambda(lambda t: tfp.distributions.Bernoulli(t[..., :1])))\n",
    "        \n",
    "        model.compile(optimizer, negloglik)\n",
    "\n",
    "        hist = model.fit(trn_x[train_index], trn_y[train_index], batch_size, epochs, validation_data=(trn_x[val_index], trn_y[val_index]), shuffle=True, verbose=0, callbacks=[early_stop])\n",
    "        loss = model.evaluate(trn_x[val_index], trn_y[val_index], verbose=0)\n",
    "\n",
    "        cross_validation_models.append(model)\n",
    "        cross_validation_losses.append(loss)\n",
    "        cross_validation_histos.append(hist)\n",
    "\n",
    "\n",
    "    ## Pick the best model from cross validation\n",
    "    idx_best = np.argmin(cross_validation_losses)\n",
    "\n",
    "    best_model = cross_validation_models[idx_best]\n",
    "    best_model.save('results/probabilistic_classification/specific/' + station + '/model' + str(r).zfill(2) + '.h5')\n",
    "    best_hist = cross_validation_histos[idx_best]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(best_hist.history['loss'], label='loss')\n",
    "    ax.plot(best_hist.history['val_loss'], label='val_loss')\n",
    "    fig.legend()\n",
    "    \n",
    "    fig.savefig('results/probabilistic_classification/specific/' + station + '/loss' + str(r).zfill(2) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0fa18c-275a-4337-90be-abf32a6df6ab",
   "metadata": {},
   "source": [
    "### Testing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ccfc0-5992-455d-8fc6-c3eba02ca1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_t = np.load(data_dir + '/' + station + '/tst_t.npy')\n",
    "tst_x = np.load(data_dir + '/' + station + '/tst_x.npy')\n",
    "tst_x = tst_x[:, id_predictors]\n",
    "tst_y = np.load(data_dir + '/' + station + '/tst_y.npy')\n",
    "\n",
    "tst_y = np.array(tst_y >= 0.1, dtype=int)\n",
    "print('Test dataset:', tst_x.shape, '--->', tst_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e08a8-e3ad-4527-bf5a-dce8273fe2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for r in range(n_runs):\n",
    "\n",
    "    model = tf.keras.models.load_model('results/probabilistic_classification/specific/' + station + '/model' + str(r).zfill(2) + '.h5', compile=False)\n",
    "    pred = model(tst_x)\n",
    "    pred = pred.mean()\n",
    "    pred = np.squeeze(pred)\n",
    "    results.append(pred)\n",
    "\n",
    "results = np.array(results)\n",
    "print('Results [runs x ensembles x predictions] =', results.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3c69e8-b70e-4a58-8eb4-a4635b238d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_all = []\n",
    "\n",
    "for r in range(n_runs):\n",
    "    \n",
    "    brier_preds = brier_score_loss(tst_y, results[r])\n",
    "\n",
    "    ## Get COSMO performance\n",
    "    px = './data/pickles/'\n",
    "    # _load this data once for each weather station\n",
    "    obsAndRef = ld(px+f'{station}.forValid.1x1.y1to7.l1to21.pickle')\n",
    "    obsAndRefLocl = obsAndRef[obsAndRef['ini'].isin([0])]\n",
    "    obsAndRefLocl = obsAndRefLocl[obsAndRefLocl['lea'].isin([4])]\n",
    "    obsAndRefLocl = obsAndRefLocl[obsAndRefLocl.tista.isin(tst_t)]\n",
    "    varis = ['TOT_PREC_delta__median']\n",
    "    obsAndRefLocl = obsAndRefLocl.filter(varis)\n",
    "    # obsAndRefLocl = obsAndRefLocl.filter(regex='^TOT_PREC_delta__p')\n",
    "    cosmo_ensemble = obsAndRefLocl.to_numpy()\n",
    "    cosmo_ensemble = np.squeeze(cosmo_ensemble)\n",
    "    cosmo_ensemble = np.array(cosmo_ensemble >= 0.1, dtype=float)\n",
    "    print(cosmo_ensemble.shape)\n",
    "    \n",
    "    brier_cosmo = brier_score_loss(tst_y, cosmo_ensemble)\n",
    "    \n",
    "    brier_skill = 1 - (brier_preds / brier_cosmo)\n",
    "    skill_all.append(brier_skill)\n",
    "    \n",
    "    \n",
    "print(station, 'Results:', 'min =', np.min(skill_all), 'median = ', np.median(skill_all), 'max =', np.max(skill_all))\n",
    "np.save('results/probabilistic_classification/specific/' + station + '/results_brier_skill.npy', skill_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0914e0-eab7-4baa-ac8e-27fe392118f0",
   "metadata": {},
   "source": [
    "## General probabilistic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20cecfd-c4d5-4e4e-9644-a4449cf8ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_depth = archs['general']['depth']\n",
    "best_units = archs['general']['units']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5785d37-beae-42c1-96ce-952f74913e43",
   "metadata": {},
   "source": [
    "### Merge all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0097bfe0-ea14-42be-856a-552e7d9e6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_x = []\n",
    "trn_y = []\n",
    "\n",
    "for station in locations:\n",
    "    \n",
    "    trn_x.append(np.load(data_dir + '/' + station + '/trn_x.npy'))\n",
    "    trn_y.append(np.load(data_dir + '/' + station + '/trn_y.npy'))\n",
    "\n",
    "trn_x = np.concatenate(trn_x)\n",
    "trn_y = np.concatenate(trn_y)\n",
    "\n",
    "## Boolean rain / no rain\n",
    "trn_y = np.array(trn_y >= 0.1, dtype=float)\n",
    "print('Train dataset:', trn_x.shape, '---> ', trn_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713f1ec3-d0cb-4495-b3df-e17b8dd3794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(n_runs):\n",
    "    \n",
    "    print('Run', r+1, '/', n_runs)\n",
    "    \n",
    "    cross_validation_models = []\n",
    "    cross_validation_losses = []\n",
    "    cross_validation_histos = []\n",
    "\n",
    "\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    for train_index, val_index in kf.split(trn_x, trn_y):\n",
    "\n",
    "        batch_size = 64\n",
    "        epochs = 256\n",
    "\n",
    "        optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
    "        negloglik = lambda y, dist: -dist.log_prob(y)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5)\n",
    "\n",
    "        activation = 'relu'\n",
    "\n",
    "        model = tf.keras.Sequential()\n",
    "\n",
    "        model.add(tf.keras.layers.InputLayer(input_shape=(trn_x.shape[1])))\n",
    "\n",
    "        for d in range(best_depth):\n",
    "            model.add(tf.keras.layers.Dense(best_units, activation=activation))\n",
    "\n",
    "        \n",
    "       ## Probabilistic output\n",
    "        model.add(tf.keras.layers.Dense(1))\n",
    "        model.add(tfp.layers.DistributionLambda(lambda t: tfp.distributions.Bernoulli(t[..., :1])))\n",
    "        \n",
    "        model.compile(optimizer, negloglik)\n",
    "\n",
    "        hist = model.fit(trn_x[train_index], trn_y[train_index], batch_size, epochs, validation_data=(trn_x[val_index], trn_y[val_index]), shuffle=True, verbose=0, callbacks=[early_stop])\n",
    "        loss = model.evaluate(trn_x[val_index], trn_y[val_index], verbose=0)\n",
    "\n",
    "        cross_validation_models.append(model)\n",
    "        cross_validation_losses.append(loss)\n",
    "        cross_validation_histos.append(hist)\n",
    "\n",
    "\n",
    "    ## Pick the best model from cross validation\n",
    "    idx_best = np.argmin(cross_validation_losses)\n",
    "\n",
    "    best_model = cross_validation_models[idx_best]\n",
    "    best_model.save('results/probabilistic_classification/general/model' + str(r).zfill(2) + '.h5')\n",
    "    best_hist = cross_validation_histos[idx_best]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(best_hist.history['loss'], label='loss')\n",
    "    ax.plot(best_hist.history['val_loss'], label='val_loss')\n",
    "    fig.legend()\n",
    "    \n",
    "    fig.savefig('results/probabilistic_classification/general/loss' + str(r).zfill(2) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a005763f-9522-44d3-9d06-af652cf03664",
   "metadata": {},
   "source": [
    "### Test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e2cc44-8f69-4229-bc36-74ee796b0afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in locations:\n",
    "    \n",
    "    tst_t = np.load(data_dir + '/' + station + '/tst_t.npy')\n",
    "    tst_x = np.load(data_dir + '/' + station + '/tst_x.npy')\n",
    "    tst_y = np.load(data_dir + '/' + station + '/tst_y.npy')\n",
    "\n",
    "    tst_y = np.array(tst_y >= 0.1, dtype=int)\n",
    "    print('Test dataset:', tst_x.shape, '--->', tst_y.shape)\n",
    "    \n",
    "    ## Load model and get predictions\n",
    "    results = []\n",
    "\n",
    "    for r in range(n_runs):\n",
    "\n",
    "        model = tf.keras.models.load_model('results/probabilistic_classification/general/model' + str(r).zfill(2) + '.h5', compile=False)\n",
    "        pred = model(tst_x)\n",
    "        pred = pred.mean()\n",
    "        pred = np.squeeze(pred)\n",
    "        results.append(pred)\n",
    "\n",
    "    results = np.array(results)\n",
    "    print('Results [runs x ensembles x predictions] =', results.shape) \n",
    "    \n",
    "\n",
    "    ## Get COSMO peformance\n",
    "    px = './data/pickles/'\n",
    "    # _load this data once for each weather station\n",
    "    obsAndRef = ld(px+f'{station}.forValid.1x1.y1to7.l1to21.pickle')\n",
    "    obsAndRefLocl = obsAndRef[obsAndRef['ini'].isin([0])]\n",
    "    obsAndRefLocl = obsAndRefLocl[obsAndRefLocl['lea'].isin([4])]\n",
    "    obsAndRefLocl = obsAndRefLocl[obsAndRefLocl.tista.isin(tst_t)]\n",
    "    varis = ['TOT_PREC_delta__median']\n",
    "    obsAndRefLocl = obsAndRefLocl.filter(varis)\n",
    "    cosmo_ensemble = obsAndRefLocl.to_numpy()\n",
    "    cosmo_ensemble = np.squeeze(cosmo_ensemble)\n",
    "    cosmo_ensemble = np.array(cosmo_ensemble >= 0.1, dtype=float)\n",
    "    print(cosmo_ensemble.shape)\n",
    "    brier_cosmo = brier_score_loss(tst_y, cosmo_ensemble)\n",
    "    \n",
    "    \n",
    "    ## Evaluate predictions\n",
    "    skill_all = []\n",
    "    for r in range(n_runs):\n",
    "\n",
    "        brier_preds = brier_score_loss(tst_y, results[r])\n",
    "        brier_skill = 1 - (brier_preds / brier_cosmo)\n",
    "        skill_all.append(brier_skill)\n",
    "    \n",
    "    print(station, 'Results:', 'min =', np.min(skill_all), 'median = ', np.median(skill_all), 'max =', np.max(skill_all))\n",
    "    np.save('results/probabilistic_classification/general/' + station + '_results_brier_skill.npy', skill_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
